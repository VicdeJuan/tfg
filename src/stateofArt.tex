% -*- root: ../main.tex -*-
% -*- dic: en_GB  -*-
\chapter{State of the art\label{sec:estado_del_arte}}

\paragraph{Abstract}

We cover here some of the basics needed to understand the real thesis of this \thisworkm. We will describe some fundamentals of the first order logic (\gls{FOL}), about his undeciability and, of course, how we can use the \gls{FOL} to prove properties of a program. Just two general properties will be covered, such as safety and liveness.

The thesis of this \thisworkm is the definition of a theory for linked list with which we can prove safety. Thus, we will define safety and some examples of how one can prove safety in simpler integer programs.

We include a brief logic of temporal logic needed for lifeness proving, but this topic is not very important in this \thisworkm.

Finally, we try to answer the question of parallelism. We will cover how can we prove safety for programs with multiple threads. We will describe some very important results used in this \thisworkm done by \citep{thesisAle}.

\section{First order logic}

\paragraph{Notation}
\label{def:notation}
We assume the usual way of representing and working with \gls{FOL}, this is
\begin{itemize}
	\item \textbf{Symbols:} $\{),(, \implies, \dimplies, \orcond , \andcond \}$
	\item \textbf{Quantifiers:} $\{\forall, \exists\}$
	\item \textbf{Constants:} $\{\true,\false\}$
	where we define $\true$ as \textit{true} and $\false$ as \textit{false}.
\end{itemize}

One could consider $\exists x(P(x))$  as an abbreviation of $\neg (\forall x(\neg P(x)))$, but for better understanding we would use both quantifiers when needed.
We could also use $(a \orcond b)$ instead of $(\neg a \implies b)$ but, again, for the better understanding those abbreviations will be used.
The same happens with $\true \equiv \neq \false$, but it is clearer when we use both symbols and not just one of them.


\paragraph{Definitions}

We are going to define some very basic concepts, needed and used during the whole \thisworkm.

\begin{defn}[Satisfiability]

A formula $F$ is satisfiable \gls{iff} there exists an interpretation $I$ such that \[I \vDash F\]
\end{defn}

\begin{defn}[Validity]

A formula $F$ is valid \gls{iff} for all interpretations $I$, \[I\vDash F\]
\end{defn}
\label{def:validity}
This 2 concepts are very important and they are very related. $F$ is valid \gls{iff} $\neg F$ is unsatisfiable. 

\begin{defn}[Theory]

A first-order theory is defined by the following components: 
\begin{itemize}
	\item Its \textbf{signature} $\Sigma$ is a set of constant, function and predicate symbols.
	\item Its set of \textbf{axioms} $\axioms$ is a set of \gls{FOL} closed formulae in which only elements from $\Sigma$ appear.
	\subitem The axioms must be 
\end{itemize}
\end{defn}

Now we define some properties a theory may have.

\begin{defn}[Completeness]

	A theory $\Sigma$ is complete \gls{iff} for every closed $\Sigma-$formula $\sigma$ we have \[(\Sigma\vDash \sigma) \orcond (\Sigma\neg\vDash \sigma) \] 
\end{defn}



\begin{defn}[Consistency]

	A theory $\Sigma$ is consistent if there is at least one $\Sigma-$interpretation.

	Another equivalent definition of consistency is:

	A theory $\Sigma$ is consistent if $\Sigma \vDash \false$
\end{defn}

If our theory is not consistent, we can have a formal proof of everything we want to prove.
We can prove that some program is correct and that is not correct, which gives us no information. 

In the other hand, completeness may not be possible to achieve because of the incompleteness theorem of Godel.



\begin{itheorem}[Godel's\IS incompleteness theorem]

\textit{Obtained from \citeapos{Godel}}

For any formal effectively generated theory T including basic arithmetical truths and also certain truths about formal provability, if T includes a statement of its own consistency then T is inconsistent.

\end{itheorem}

% TODO: 
\textcolor{red}{Conclusions of this theorem}

%From this theorem we see that we can't have a complete and consistent theory while including basic arithmetical truths. Normally, we would have to choose.


\subsection{Decidability}

\begin{defn}[Decidability\IS of theory]

A theory $\Sigma$ is decidable if $\Sigma \vDash F$ is decidable, for every $\Sigma-$formula.

\end{defn}

\begin{defn}[Decidability\IS of formula]

$F$ a $\Sigma-$formula is decidable if there is an algorithm that always terminates with ``yes'' if $F$ is valid in $\Sigma$ ($\Sigma$-valid) or ``no'' if $F$ is not $\Sigma-$valid.
\end{defn}

Decidability is a very desirable property but because \gls{FOL} (the theory with no axioms) is undecidable in general, we may not have decidability in the theory we are working on.

Decidability is a stronger property of a theory than completeness. 


\paragraph{Example:}

\index{Theory\IS of equality}
\label{theory:equality}
We are going to define the theory of equality, because it is the simplest first-order theory and it is the only theory included in SPASS.\footnote{We will talk about SPASS in section \ref{def:SPASS}}

The signature of the theory is:

\[\Sigma_e:\{=,a,b,c,...\}\]

and it's axioms are:

\begin{description}
	\item[Reflexivity:	] $\forall x, x=X$
	\item[Symmetry:	] $\forall x,y x=y \implies y=x$
	\item[Transitivity:	] $\forall x,y,z x=y \andcond y=z \implies x=z$
	\item[Congruence:	]
	\subitem \textbf{Function:} For each function $f$
	\[\forall \gor{x},\gor{y} \left( \bigwedge_{i=1}^n x_i = y_i \right) \implies f(\gor{x}) = f(\gor{y})\]
	\subitem \textbf{Predicate:} For each predicate $P$
	\[\forall \gor{x},\gor{y} \left( \bigwedge_{i=1}^n x_i = y_i \right) \implies f(\gor{x}) \dimplies f(\gor{y})\]

	This 2 ``axioms'' are not axioms but an \concept{Axiom\IS schema}, because there is one axiom for each function $f$ we want to use.
\end{description}

This is a decidable theory as Leopold Löwenheim proved in 1915 \cite{EqualityIsDecidable}.


\subsection{Second-order logic (SOL)}

In \gls{SOL} the quantifiers can be used to quantify functions and/or predicates. This gives lot of possibilities to reason about the world and problems but adds lot of complexity.

In the theory of equality (\ref{theory:equality}) we could have defined axioms of congruence by:

\[\forall f \left( \forall \gor{x},\gor{y} \left( \bigwedge_{i=1}^n x_i = y_i \right) \implies f(\gor{x}) = f(\gor{y}) \right)\]
\[\forall P \left( \forall \gor{x},\gor{y} \left( \bigwedge_{i=1}^n x_i = y_i \right) \implies P(\gor{x}) \dimplies P(\gor{y}) \right)\]

This is a simpler way of writing but a more complex way of reasoning.

Because it is not possible to reason automatically in theories of second order, we haven't used any second-order logic and we won't get deeper into it.




\section{Program correctness}

We are finally ready to apply this concepts to a real word problem, proving properties of programs.
The remaining task is to define the framework and the conventions we use to prove formally properties of programs.

There are two forms of proving properties. \textbf{Partial correctness} which assert that certain states can not occur during an execution (tipically error states) or \textbf{Total correctness} which assert that some state is eventually reached during any execution. For total correctness we will have to introduce what \textbf{temporal logic} is and its ``quantifiers'' always and eventually.


\paragraph{Simplified Programming Language (SPL)}

\label{def:SPL}

We are going to use the \gls{SPL} used in \citep[II.1]{thesisAle}. This is a very simple way of representing programs, general enough so other programming languages fit in this definition. 


% TODO: Mention the assume.
\subsection{Partial correctness (Safety)}



\subsection{Total correctness (Lifeness)}

What is proving lifeness, for what we need temporal logic.

\subsubsection{Temporal logic}

Basics on temporal logics

\subsubsection{Lifeness examples}

Now we can give some examples of lifeness examples. 
% TODO:
\textcolor{red}{Conjecture César told me}


\subsection{Parametrized systems}

All the exposed until this point is just for one thread executing, and that is not a very interesting problem. 
The correctness of just one thread executing a program is easy because it sequentiality.
The useful to solve problem is whenever we have multiple threads executing the same program. And this number of threads executing the same problems may be bounded or not. 

We are going to study those cases.

To do so, we need to represent a program executed by multiple threads. We can parametrize by the thread. Typically the threads would be $i$,$j$,$k_0$,$k_1$,... 

% TODO:
\textcolor{red}{Complete from book}

\subsubsection{Finite number of threads}


\subsubsection{Arbitrary number of threads}

In this kind of programs, we have, for example, the web servers may not have a bound of the number of clients the can accept. Can we prove correctness when an unbounded number of processes are using the same shared variable?

A recent research \citeapos{paperParametrizedInvariants} has proven a very important result. We will use this result.

\textcolor{red}{Complete}
\begin{itheorem}[Bound an arbitrary number of threads]
We just need to prove 3 things. Initiation, self consecution and others consecution.
\end{itheorem}
\label{thm:biggest}

Using this powerful result, we have reduced an arbitrary number of processes sharing the same variable to a finite number of threads sharing the variable. This is a very powerful result and we will come to it later on, when we talk about the proofs we have done. 
